{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import minerl\n",
    "import logging\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path\n",
    "import collections\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import torch.nn.utils as torch_utils\n",
    "from copy import deepcopy\n",
    "\n",
    "logging.disable(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def get_and_format_data(path_list):\n",
    "    all_data = []\n",
    "\n",
    "    for i, path in enumerate(path_list):\n",
    "        action_values = np.load(path + \"/rendered.npz\", allow_pickle=True)\n",
    "        min_len = len(action_values['reward'])\n",
    "\n",
    "        frames = extract_frames(path + \"/recording.mp4\")\n",
    "        frames = frames[:min_len]\n",
    "\n",
    "        aligned_data = [(frames[i], {k: v[i] for k, v in action_values.items()}) for i in range(min_len)]\n",
    "\n",
    "        aligned_data = aligned_data[:((len(aligned_data) // 128) * 128)]\n",
    "        all_data.append(aligned_data)\n",
    "\n",
    "    return [pair for video in all_data for pair in video]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = Path(\"MineRLTreechop-v0/\")\n",
    "path_list = [\"MineRLTreechop-v0/\" + f.name for f in directory_path.iterdir()]\n",
    "\n",
    "data = get_and_format_data(path_list)\n",
    "\n",
    "\n",
    "print(f\"Amount of episodes: {len(data)}\")\n",
    "print(f\"\\nEach element in data is a tuple of form: (frames, action-value dict)\")\n",
    "print(f\"Shape of pov frame: {data[0][0].shape}\")\n",
    "\n",
    "print(\"\\nActions:\")\n",
    "for key in data[0][1].keys():\n",
    "    print(\"  \" + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx in range(20, 30):\n",
    "\n",
    "    plt.imshow(data[idx][0])\n",
    "    plt.title(\"Sample Frame from Dataset\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    print(data[idx][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"video_and_actions.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"video_and_actions.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# data = data[:250000]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionManager:\n",
    "    \"\"\"Main Minecraft action wrapper with improved camera quantization.\"\"\"\n",
    "\n",
    "    def __init__(self, device, c_action_magnitude=10):\n",
    "        self.device = device\n",
    "        self.c_action_magnitude = c_action_magnitude\n",
    "\n",
    "        self.zero_action = OrderedDict([('action$attack', 0),\n",
    "                                        ('action$back', 0),\n",
    "                                        ('action$camera', np.array([0., 0.])),\n",
    "                                        ('action$forward', 0),\n",
    "                                        ('action$jump', 0),\n",
    "                                        ('action$left', 0),\n",
    "                                        ('action$right', 0),\n",
    "                                        ('action$sneak', 0),\n",
    "                                        ('action$sprint', 0)])\n",
    "\n",
    "        # camera discretization:\n",
    "        self.camera_dict = OrderedDict([\n",
    "            ('action$turn_up', np.array([-c_action_magnitude, 0.])),\n",
    "            ('action$turn_down', np.array([c_action_magnitude, 0.])),\n",
    "            ('action$turn_left', np.array([0., -c_action_magnitude])),\n",
    "            ('action$turn_right', np.array([0., c_action_magnitude]))\n",
    "        ])\n",
    "\n",
    "        self.fully_connected_no_camera = ['action$attack', 'action$back', 'action$forward', 'action$jump', 'action$left', 'action$right', 'action$sprint']\n",
    "        self.camera_actions = list(self.camera_dict.keys())\n",
    "        self.fully_connected = self.fully_connected_no_camera + self.camera_actions\n",
    "\n",
    "        # Action constraints\n",
    "        self.exclude = [('action$forward', 'action$back'), ('action$left', 'action$right'), ('action$attack', 'action$jump'), ('action$turn_up', 'action$turn_down', 'action$turn_left', 'action$turn_right')]\n",
    "        self.only_if = [('action$sprint', 'action$forward')]\n",
    "\n",
    "        # If more than 3 actions, these are the order by which to remove extra actions\n",
    "        self.remove_size = 3\n",
    "        self.remove_first_list = ['action$sprint', 'action$left', 'action$right', 'action$back',\n",
    "                                  'action$turn_up', 'action$turn_down', 'action$turn_left', 'action$turn_right',\n",
    "                                  'action$attack', 'action$jump', 'action$forward']\n",
    "\n",
    "        self.fully_connected_list = list(product(range(2), repeat=len(self.fully_connected)))\n",
    "\n",
    "        # Remove invalid action combinations\n",
    "        remove = []\n",
    "        for el in self.fully_connected_list:\n",
    "            for tuple_ in self.exclude:\n",
    "                if sum([el[self.fully_connected.index(a)] for a in tuple_]) > 1:\n",
    "                    if el not in remove:\n",
    "                        remove.append(el)\n",
    "            for a, b in self.only_if:\n",
    "                if el[self.fully_connected.index(a)] == 1 and el[self.fully_connected.index(b)] == 0:\n",
    "                    if el not in remove:\n",
    "                        remove.append(el)\n",
    "            if sum(el) > self.remove_size:\n",
    "                if el not in remove:\n",
    "                    remove.append(el)\n",
    "\n",
    "        for r in remove:\n",
    "            self.fully_connected_list.remove(r)\n",
    "\n",
    "        self.action_list = []\n",
    "        for el in self.fully_connected_list:\n",
    "            new_action = copy.deepcopy(self.zero_action)\n",
    "            for key, value in zip(self.fully_connected, el):\n",
    "                if key in self.camera_actions:\n",
    "                    if value:\n",
    "                        new_action['action$camera'] = self.camera_dict[key]\n",
    "                else:\n",
    "                    new_action[key] = value\n",
    "            self.action_list.append(new_action)\n",
    "\n",
    "        self.num_action_ids_list = [len(self.action_list)]\n",
    "        self.act_continuous_size = 0\n",
    "\n",
    "    def quantize_camera(self, camera):\n",
    "        \"\"\"Snap camera movement to the nearest discrete step.\"\"\"\n",
    "        camera_steps = np.array([-40, -20, -10, 5, 0, 5, 10, 20, 40])\n",
    "\n",
    "        camera[0] = camera_steps[np.abs(camera_steps - camera[0]).argmin()]\n",
    "        camera[1] = camera_steps[np.abs(camera_steps - camera[1]).argmin()]\n",
    "        return camera\n",
    "\n",
    "    def get_action(self, id):\n",
    "        \"\"\"Retrieve an action by ID\"\"\"\n",
    "        a = copy.deepcopy(self.action_list[int(id)])\n",
    "        a['action$camera'] += np.random.normal(0., 0.5, 2)\n",
    "        a = dict(a)\n",
    "\n",
    "        for key, _ in a.items():\n",
    "            if key == 'action$camera':\n",
    "                a[key] = np.array(a[key])\n",
    "            else:\n",
    "                a[key] = np.int64(a[key])\n",
    "        return a\n",
    "\n",
    "    def get_id(self, action):\n",
    "        \"\"\"Convert an action into a discrete ID.\"\"\"\n",
    "\n",
    "        action = copy.deepcopy(action)\n",
    "        # action['action$camera'] = self.quantize_camera(action['action$camera'])\n",
    "\n",
    "        for tuple_actions in self.exclude:\n",
    "            if len(tuple_actions) == 2:\n",
    "                a, b = tuple_actions\n",
    "\n",
    "                if action[a] and action[b]:\n",
    "                    action[b] = 0\n",
    "\n",
    "        for a, b in self.only_if:\n",
    "            if not action[b]:\n",
    "                if action[a]: action[a] = 0\n",
    "\n",
    "        # discretize 'camera':\n",
    "        camera = action['action$camera']\n",
    "        camera_action_amount = 0\n",
    "        if - self.c_action_magnitude / 3. < camera[0] < self.c_action_magnitude / 3.:\n",
    "            action['action$camera'][0] = 0.\n",
    "            if - self.c_action_magnitude / 3. < camera[1] < self.c_action_magnitude / 3.:\n",
    "                action['action$camera'][1] = 0.\n",
    "            else:\n",
    "                camera_action_amount = 1\n",
    "                action['action$camera'][1] = self.c_action_magnitude * np.sign(camera[1])\n",
    "        else:\n",
    "            camera_action_amount = 1\n",
    "            action['action$camera'][0] = self.c_action_magnitude * np.sign(camera[0])\n",
    "\n",
    "            action['action$camera'][1] = 0.\n",
    "\n",
    "        for a in self.remove_first_list:\n",
    "            if sum([action[key] for key in self.fully_connected_no_camera]) > (self.remove_size - camera_action_amount):\n",
    "                if a in self.camera_actions:\n",
    "                    action['action$camera'] = np.array([0., 0.])\n",
    "                    camera_action_amount = 0\n",
    "                else:\n",
    "                    action[a] = 0\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        for key in self.camera_actions:\n",
    "            action[key] = 0\n",
    "        for key, val in self.camera_dict.items():\n",
    "            if (action['action$camera'] == val).all():\n",
    "                action[key] = 1\n",
    "                break\n",
    "\n",
    "        non_separate_values = tuple(action[key] for key in self.fully_connected)\n",
    "        return self.fully_connected_list.index(non_separate_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ChopTreeAgent(nn.Module):\n",
    "    def __init__(self, output_dim=10, num_frames=2):\n",
    "        super(ChopTreeAgent, self).__init__()\n",
    "\n",
    "        input_channels = 3 * num_frames\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),  # (32, 20, 20)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # (64, 9, 9)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # (64, 7, 7)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)  # Output: 8 discrete and 2 continuous\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "\n",
    "class BCDataset(Dataset):\n",
    "    def __init__(self, aligned_data, num_frames=2, transform=None):\n",
    "        \"\"\"\n",
    "        aligned_data: list of (frame, action_dict) pairs\n",
    "        transform: torchvision transform to apply to each frame\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.data = aligned_data\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        self.discrete_keys = ['forward', 'left', 'back', 'right', 'jump', 'sneak', 'sprint', 'attack']\n",
    "        self.continuous_key = 'camera'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.num_frames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = []\n",
    "        camera_actions = []\n",
    "\n",
    "        for i in range(self.num_frames):\n",
    "            frame, action_dict  = self.data[idx + i]\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "\n",
    "            # Average camera value over frames for smoothness\n",
    "            camera_action = action_dict.get(f\"action${self.continuous_key}\", [0.0, 0.0])\n",
    "\n",
    "            if not isinstance(camera_action, list): camera_action = camera_action.tolist()\n",
    "            camera_actions.append(camera_action)\n",
    "        \n",
    "        stacked_frames = torch.cat(frames, dim=0)  # Shape: (3 * num_frames, 84, 84)\n",
    "        avg_camera_action = torch.tensor(camera_actions, dtype=torch.float32).mean(dim=0)\n",
    "\n",
    "        _, action_dict = self.data[idx + self.num_frames - 1]  # Use last frame's action\n",
    "        discrete_actions = [float(action_dict.get(f\"action${k}\", 0)) for k in self.discrete_keys]\n",
    "\n",
    "        action_vector = torch.tensor(discrete_actions + avg_camera_action.tolist(), dtype=torch.float32)\n",
    "\n",
    "        return stacked_frames, action_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 0.99\n",
    "num_epochs = 40\n",
    "lr = 0.0005\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "action_manager = ActionManager(device)\n",
    "output_dim = len(action_manager.action_list)\n",
    "\n",
    "dataset = BCDataset(data, action_manager=action_manager, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "model = ChopTreeAgent(input_channels=3, output_dim=output_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for frames, action_vectors, rewards, rc_frames in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        frames, action_vectors = frames.to(device), action_vectors.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(frames)\n",
    "\n",
    "        # Compute losses\n",
    "        action_loss = loss_fn(outputs, action_vectors.long())\n",
    "\n",
    "        weighted_loss = action_loss * (1 + rewards.squeeze())\n",
    "        loss = weighted_loss.mean()\n",
    "\n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * frames.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"minecraft_tree_agent.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model weights saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "print(f\"Model weights loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_obs(obs):\n",
    "    \"\"\"\n",
    "    Converts the environment observation into a 4D tensor suitable for the model\n",
    "    \"\"\"\n",
    "    frame = obs[\"pov\"]\n",
    "\n",
    "    transform_pipeline = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.ToTensor(),  # scales pixels to [0,1]\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    frame = transform_pipeline(frame)\n",
    "    return frame.unsqueeze(0) # Add a batch dimension\n",
    "\n",
    "\n",
    "action_keys = ['forward', 'left', 'back', 'right', 'jump', 'sneak', 'sprint', 'attack']\n",
    "\n",
    "def build_action_dict(actions, env):\n",
    "    \"\"\"\n",
    "    Create a full action dictionary for the MineRL environment\n",
    "    \"\"\"\n",
    "    ac = env.action_space.no_op()  # Start with a no-op action dictionary\n",
    "\n",
    "    for key in action_keys:\n",
    "        ac[key] = actions[\"action$\" + key]\n",
    "\n",
    "    ac['camera'] = actions[\"action$camera\"]\n",
    "    return ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "env = gym.make('MineRLObtainDiamondShovel-v0')\n",
    "\n",
    "env.seed(1)\n",
    "obs = env.reset()\n",
    "\n",
    "hidden_state = None\n",
    "steps = 0\n",
    "done = False\n",
    "\n",
    "try:\n",
    "    while not done:\n",
    "        # Preprocess observation from the environment and stack frames\n",
    "        state = preprocess_obs(obs)\n",
    "\n",
    "        # Get model output and convert to actions\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(state)\n",
    "\n",
    "        action_id = torch.argmax(output, dim=1).item()\n",
    "\n",
    "        action = action_manager.get_action(action_id)\n",
    "\n",
    "        print(action)\n",
    "\n",
    "        action = build_action_dict(action, env)\n",
    "\n",
    "        # Step the environment using the agent's action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "        steps += 1\n",
    "        if steps == 10000:\n",
    "            done = True\n",
    "finally:\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
